{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1071417-a685-4e50-8d6a-349f84947511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "import evofr as ef\n",
    "from numpyro.infer.autoguide import AutoDelta, AutoMultivariateNormal\n",
    "from pathlib import Path\n",
    "import os\n",
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "from jax.nn import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import Day, BDay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4293fd4c-1aed-4255-8472-d7148b72e65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/eabousam/freq_dynamics/ncov-forecasting-fit/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69434412-e7a5-4653-b355-092140960a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/eabousam/freq_dynamics/ncov-forecasting-fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a5b2c1-b678-4c94-9bfd-52ace164fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting sequences data from commit dates\n",
    "dates = ['2022-04-15','2022-04-22','2022-04-29','2022-05-06',\n",
    "         '2022-05-13','2022-05-20','2022-05-27','2022-06-03',\n",
    "         '2022-06-10','2022-06-17','2022-06-24','2022-06-30']\n",
    "\n",
    "#specifying location to run model on\n",
    "locations = [\"USA\", \"United Kingdom\", \"Brazil\",\"Australia\",\"South Africa\", \"Japan\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02551a2a-b650-4325-9c23-a2f59f201455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model\n",
    "seed_L = 14\n",
    "forecast_L = 14\n",
    "ps = [0.95, 0.8, 0.5]\n",
    "# Get delays\n",
    "v_names = ['Delta', \n",
    "           'Omicron 21L', \n",
    "           'Omicron 21K', \n",
    "           'Omicron 22A', \n",
    "           'Omicron 22B', \n",
    "           'Omicron 22C', \n",
    "           'other']\n",
    "\n",
    "gen = ef.pad_delays(\n",
    "    [ef.discretise_gamma(mn=4.4, std=1.2), # Delta\n",
    "     ef.discretise_gamma(mn=3.1, std=1.2), # Omicron 21L\n",
    "     ef.discretise_gamma(mn=3.1, std=1.2), # Omicron 21K #3.1 std 1.2 \n",
    "     ef.discretise_gamma(mn=3.1, std=1.2), # Omicron 22A\n",
    "     ef.discretise_gamma(mn=3.1, std=1.2), # Omicron 22B\n",
    "     ef.discretise_gamma(mn=3.1, std=1.2), # Omicron 22C\n",
    "     ef.discretise_gamma(mn=4.4, std=1.2)] # Other\n",
    "    )\n",
    "\n",
    "delays = ef.pad_delays([ef.discretise_lognorm(mn=3.1, std=1.0)])  \n",
    "\n",
    "basis_fn = ef.Spline(order = 4, k = 10)\n",
    "#dict for models\n",
    "model_type = dict()\n",
    "\n",
    "\n",
    "#Fixed Growth Advantage model for variants\n",
    "model_type['FGA'] = ef.RenewalModel(gen, delays, seed_L, forecast_L, k=10,\n",
    "                RLik = ef.FixedGA(0.1), #Likelihood on effective reproduction number (GARW depend on R and gen time)\n",
    "                CLik = ef.ZINegBinomCases(0.05), #Case Likelihood\n",
    "                SLik = ef.DirMultinomialSeq(100), #Sequence Likelihood\n",
    "                v_names = v_names,\n",
    "                basis_fn = basis_fn)\n",
    "\n",
    "#Varying Growth Advantage Random Walk Model\n",
    "model_type['GARW'] = ef.RenewalModel(gen, delays, seed_L, forecast_L, k=10,\n",
    "                RLik = ef.GARW(0.1,0.01, prior_family='Normal'), #Likelihood on effective reproduction number (GARW depend on R and gen time)\n",
    "                CLik = ef.ZINegBinomCases(0.05), #Case Likelihood\n",
    "                SLik = ef.DirMultinomialSeq(100), #Sequence Likelihood\n",
    "                v_names = v_names,\n",
    "                basis_fn = basis_fn)\n",
    "\n",
    "#Multinomial Logistic regression model\n",
    "model_type['MLR'] = ef.MultinomialLogisticRegression(tau=4.2)\n",
    "\n",
    "#forecast\n",
    "#forecast_LL = 28\n",
    "#gen_p = ef.discretise_gamma(mn=3.1, std=1.2)\n",
    "#Piantham model\n",
    "model_type['Piantham'] = ef.models.PianthamModel(gen = ef.discretise_gamma(mn=3.1, std=1.2), forecast_L = 28)\n",
    "\n",
    "\n",
    "\n",
    "# defining inference method\n",
    "svi = ef.InferMAP(iters = 50_000, lr = 4e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14aba05b-695d-43d0-a95b-45d10e61f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_forecast(seq_count_date, date):\n",
    "    \n",
    "    #defining forecast and nowcast dates from pivot date\n",
    "    forecast_dates = pd.to_datetime(pd.unique(pd.date_range(start = date, periods=14, freq ='D') + Day(1))).astype(str)\n",
    "    nowcast_dates = pd.to_datetime(pd.unique(pd.date_range(end = date, periods=14, freq ='D'))).astype(str)\n",
    "\n",
    "    \n",
    "    #defining prediction period for nowcasting and forecasting\n",
    "    pred_dates = pd.to_datetime(pd.unique(pd.date_range(start = date, periods=28, freq ='D') - Day(13))).astype(str)\n",
    "    \n",
    "    #defining recent_dates (7) from pivot date\n",
    "    recent_dates = pd.to_datetime(pd.unique(pd.date_range(end = date, periods=7, freq ='D') - Day(14))).astype(str)\n",
    "\n",
    "\n",
    "    #computing the frequency of variants for each location\n",
    "    seq_count_date['total_seq'] = seq_count_date.groupby(['date', 'location'])['sequences'].transform('sum')\n",
    "    seq_count_date['freq'] = seq_count_date['sequences']/seq_count_date['total_seq']\n",
    "    \n",
    "    \n",
    "    #Computing the mean frequency for recent dates\n",
    "    seq_count_mean = seq_count_date[seq_count_date.date.isin(recent_dates)].groupby([\"variant\", \"location\"])[\"freq\"].mean().reset_index()\n",
    "    \n",
    "    \n",
    "    #Adding pred_dates to date column for each location and variant\n",
    "    sc_s = []\n",
    "    for d in pred_dates:\n",
    "        sc_ = seq_count_mean.copy()\n",
    "        #adding dates column\n",
    "        sc_[\"date\"] = d\n",
    "        sc_s.append(sc_)\n",
    "    sc = pd.concat(sc_s).sort_values(by=[\"location\", \"variant\", \"date\"])\n",
    "    #adding nowcast and forecast columns\n",
    "    sc['median_freq_nowcast'] = sc['freq']\n",
    "    sc['median_freq_forecast'] = sc['freq']\n",
    "    #matching dates for nowcast and forecast\n",
    "    sc.loc[sc.date.isin(forecast_dates),'median_freq_nowcast'] = np.nan\n",
    "    sc.loc[sc.date.isin(nowcast_dates),'median_freq_forecast'] = np.nan\n",
    "\n",
    "    return sc.reset_index(drop=True)\n",
    "\n",
    "\n",
    "for date in dates:\n",
    "    seq_count_date = pd.read_csv(f\"data/time_stamped/{date}/seq_counts_{date}.tsv\", sep=\"\\t\")\n",
    "    naive_pred = naive_forecast(seq_count_date, date)\n",
    "    #create files for estimates for each country and pivot date\n",
    "    for location in locations:\n",
    "        filepath = f'plot-est/cast_estimates_full_dummy/{location}/'\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "        naive_pred[naive_pred.location == location].to_csv(filepath+f\"freq_full_{date}.csv\", index = False)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65188165-d416-4e1e-9498-1bad24acfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export posterior frequencies forecast and no forecast\n",
    "def save_freq(samples, variant_data, ps, forecast_date, name, filepath):\n",
    "    #only need last 14 days\n",
    "    freq_now = pd.DataFrame(ef.get_freq(samples, variant_data, ps, name=name, forecast = False))\n",
    "    #Get the freq dates nowcast period\n",
    "    nowcast_dates = variant_data.dates[-14:]\n",
    "    freq_now = freq_now[freq_now['date'].isin(nowcast_dates)]\n",
    "    freq_fr = pd.DataFrame(ef.get_freq(samples, variant_data, ps, name=name, forecast = True))\n",
    "    freq_merged = pd.concat([freq_now, freq_fr])\n",
    "    #freq_now = freq_now.rename(columns = {'median_freq':'median_freq_nowcast','freq_upper_95':'freq_nowcast_upper_95'}, inplace = False)\n",
    "    \n",
    "    #rename intervals\n",
    "    freq_merged = freq_merged.rename(columns = {'median_freq':'median_freq_nowcast','freq_upper_95':'freq_nowcast_upper_95'}, inplace = False)\n",
    "\n",
    "    freq_merged.to_csv(f'{filepath}/freq_full_{forecast_date}.csv', index = False)\n",
    "    #freq_now.to_csv(f'{filepath}/freq_nowcast_{forecast_date}.csv', index = False)\n",
    "    #freq_fr.to_csv(f'{filepath}/freq_forecast_{forecast_date}.csv', index = False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce44710-45a0-43ee-bc33-6f8e2bfe8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #export posterior frequencies forecast and no forecast\n",
    "def save_mlr_freq(samples, variant_data, ps, forecast_date, name, filepath):\n",
    "\n",
    "    freq_fr = pd.DataFrame(ef.get_freq(samples, variant_data, ps, name=name, forecast = True))\n",
    "\n",
    "    #rename intervals\n",
    "    freq_fr = freq_fr.rename(columns = {'median_freq':'median_freq_nowcast','freq_upper_95':'freq_nowcast_upper_95'}, inplace = False)\n",
    "\n",
    "    freq_fr.to_csv(f'{filepath}/freq_full_{forecast_date}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58d1f10b-fb79-40ad-bfc6-16547bda01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_frequencies(samples, mlr, forecast_L):\n",
    "    \"\"\"\n",
    "    Use posterior beta to forecast posterior frequenicies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Making feature matrix for forecasting\n",
    "    last_T = samples[\"freq\"].shape[1]\n",
    "    \n",
    "    X = mlr.make_ols_feature(start=last_T, stop=last_T + forecast_L)\n",
    "    # Posterior beta\n",
    "    beta = jnp.array(samples[\"beta\"])\n",
    "    \n",
    "    # Matrix multiplication by sample\n",
    "    dot_by_sample = vmap(jnp.dot, in_axes=(None, 0), out_axes=0)\n",
    "    logits = dot_by_sample(X, beta) # Logit frequencies by variant\n",
    "    return softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bdf8852-19e2-4f86-81c2-ea64eebd75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Growth Advantage\n",
    "\n",
    "def get_gr_adv(samples, variant_data, ps, name, date, site, filepath):\n",
    "\n",
    "    growth_adv = pd.DataFrame(ef.posterior.get_site_by_variant(samples, variant_data, ps, name=name, site=site, forecast=False))\n",
    "\n",
    "    #get growth advantage relative to BA.1 Omicron 21K\n",
    "    \n",
    "    \n",
    "    growth_adv.to_csv(f'{filepath}/full_growth_advantages_{date}.csv',index = False, header = True)\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ebe1d8-bbd4-4ed3-aa5b-a102f0f58f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlr_gr_adv(samples, variant_data, ps, name,date, filepath):\n",
    "    \n",
    "\n",
    "    growth_adv = pd.DataFrame(ef.posterior.get_growth_advantage(samples, variant_data, ps, name=name, rel_to=\"other\"))\n",
    "    growth_adv.to_csv(f'{filepath}/full_growth_advantages_{date}.csv',index = False, header=True)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "151afdcc-22b4-4a1f-9ee0-e910bb6ec68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 12:42:10.156922: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 4m59.598s\n",
      "\n",
      "********************************\n",
      "[Compiling module jit_prim_fun.5608] Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "models = ['Piantham','GARW','MLR', 'FGA']\n",
    "\n",
    "\n",
    "#function to input models \n",
    "def model_run(model):\n",
    "    for location in locations:\n",
    "        for date in dates:\n",
    "            #Output file path\n",
    "            filepath = f\"./plot-est/cast_estimates_full_{model}/{location}\"    \n",
    "            if not os.path.exists(filepath):\n",
    "                os.makedirs(filepath)\n",
    "            #read sequences\n",
    "            raw_seq = pd.read_csv(f\"data/time_stamped/{date}/seq_counts_{date}.tsv\", sep=\"\\t\")\n",
    "            raw_cases = pd.read_csv(f\"data/time_stamped/{date}/case_counts_{date}.tsv\", sep=\"\\t\")\n",
    "            raw_cases = raw_cases[raw_cases.location == location]\n",
    "            raw_seq = raw_seq[raw_seq.location == location]\n",
    "            if len(raw_cases)==0:\n",
    "                continue\n",
    "\n",
    "            #defining variant data (freq and seq)\n",
    "            if model == 'MLR':\n",
    "                variant_data = ef.VariantFrequencies(raw_seq)\n",
    "                posterior = svi.fit(model_type[model], variant_data)\n",
    "                posterior.samples[\"freq_forecast\"] = forecast_frequencies(posterior.samples, model_type[model], 28)\n",
    "                save_mlr_freq(posterior.samples, variant_data, ps, date, location, filepath)\n",
    "                samples = posterior.samples\n",
    "                #get_mlr_gr_adv(samples, variant_data, ps, location, date, filepath)\n",
    "                \n",
    "            if model == 'Piantham':\n",
    "                variant_data = ef.VariantFrequencies(raw_seq)\n",
    "                posterior = svi.fit(model_type[model], variant_data)\n",
    "                samples = posterior.samples\n",
    "                save_freq(posterior.samples, variant_data, ps, date, location, filepath)\n",
    "                #get_mlr_gr_adv(samples, variant_data, ps, location, date, filepath)\n",
    "            if model == 'FGA':\n",
    "                variant_data = ef.CaseFrequencyData(raw_cases=raw_cases, raw_seq=raw_seq)\n",
    "                posterior = svi.fit(model_type[model], variant_data)\n",
    "                #saving posterior frequencies\n",
    "                samples = posterior.samples\n",
    "                save_freq(posterior.samples, variant_data, ps, date, location, filepath)\n",
    "                #saving growth advantages\n",
    "                #get_mlr_gr_adv(samples, variant_data, ps, location, date, filepath)      \n",
    "            if model == 'GARW':\n",
    "                variant_data = ef.CaseFrequencyData(raw_cases=raw_cases, raw_seq=raw_seq)\n",
    "                posterior = svi.fit(model_type[model], variant_data)\n",
    "                #saving posterior frequencies\n",
    "                samples = posterior.samples\n",
    "                save_freq(posterior.samples, variant_data, ps, date, location, filepath)\n",
    "                #saving growth advantages\n",
    "                #get_gr_adv(samples, variant_data, ps, location, date, \"ga\", filepath)\n",
    "\n",
    "                \n",
    "            \n",
    "    return None\n",
    " \n",
    "for model in models:\n",
    "    model_run(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
